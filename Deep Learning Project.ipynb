{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Developed by: Mandy Sack\n",
    "August 2019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "#first, verify you have the files in the correct location\n",
    "print(os.path.isdir(\"social_honeypot_icwsm_2011\"))\n",
    "print(os.path.isfile(\"social_honeypot_icwsm_2011\\content_polluters_classified.csv\"))\n",
    "print(os.path.isfile(\"social_honeypot_icwsm_2011\\legitimate_users_classified.csv\"))\n",
    "\n",
    "bDcolnames=['UserID','CreatedAt','CollectedAt', 'NumberOfFollowings', 'NumberOfFollowers', 'NumberOfTweets','LengthOfScreenName','LengthOfDescriptionInUserProfile','BadUser'] \n",
    "badData = pd.read_csv(\"social_honeypot_icwsm_2011\\content_polluters_classified.csv\", names=bDcolnames)\n",
    "\n",
    "nDcolnames=['UserID','CreatedAt','CollectedAt', 'NumberOfFollowings', 'NumberOfFollowers', 'NumberOfTweets','LengthOfScreenName','LengthOfDescriptionInUserProfile','BadUser']\n",
    "normalData = pd.read_csv(\"social_honeypot_icwsm_2011\\legitimate_users_classified.csv\", names=nDcolnames)\n",
    "\n",
    "#TODO: make this configurable to enable your own data to be passed in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(badData.head(3))\n",
    "print(normalData.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert classification of 0 or 1 based on if good or bad user\n",
    "#TODO- this does not change accurately once the two datasets are merged\n",
    "#badData.insert(8, \"Bad User\", 1, True) \n",
    "#print(badData.head(3))\n",
    "\n",
    "#normalData.insert(8,\"Bad User\",0, True)\n",
    "#print(normalData.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the Collected column to a datetime object \n",
    "#badData.CollectedAt = pd.to_datetime(badData.CollectedAt)\n",
    "#normalData.CollectedAt = pd.to_datetime(normalData.CollectedAt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the data together for 1 dataset based on CollectedAt time & save to file\n",
    "#mergedData = pd.merge_ordered(badData, normalData, fill_method='ffill', on='CollectedAt')\n",
    "#mergedData = pd.merge_ordered(badData, normalData, fill_method='ffill', left_on='CollectedAt', right_on='CollectedAt')\n",
    "mergedData = pd.concat([badData,normalData])\n",
    "mergedData.sort_values(by=['CollectedAt'], inplace=True)\n",
    "mergedData.to_csv(\"social_honeypot_icwsm_2011\\mergedData_classified.csv\")\n",
    "mergedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Explore badData dataset seperately\n",
    "print(\"badData shape\")\n",
    "print(format(badData.shape))\n",
    "print(\"\\n\")\n",
    "print(\"badData NumberOfFollowings describe\")\n",
    "print(badData['NumberOfFollowings'].describe())\n",
    "print(\"\\n\")\n",
    "print(\"badData NumberOfFollowers describe\")\n",
    "print(badData['NumberOfFollowers'].describe())\n",
    "print(\"\\n\")\n",
    "print(\"badData NumberOfTweets describe\")\n",
    "print(badData['NumberOfTweets'].describe())\n",
    "print(\"\\n\")\n",
    "print(\"badData LengthOfScreenName describe\")\n",
    "print(badData['LengthOfScreenName'].describe())\n",
    "print(\"\\n\")\n",
    "print(\"badData LengthOfDescriptionInUserProfile describe\")\n",
    "print(badData['LengthOfDescriptionInUserProfile'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore normalData dataset seperately\n",
    "print(\"normalData shape\")\n",
    "print(format(normalData.shape))\n",
    "print(\"\\n\")\n",
    "print(\"normalDataData NumberOfFollowings describe\")\n",
    "print(normalData['NumberOfFollowings'].describe())\n",
    "print(\"\\n\")\n",
    "print(\"normalDataData NumberOfFollowers describe\")\n",
    "print(normalData['NumberOfFollowers'].describe())\n",
    "print(\"\\n\")\n",
    "print(\"normalDataData NumberOfTweets describe\")\n",
    "print(normalData['NumberOfTweets'].describe())\n",
    "print(\"\\n\")\n",
    "print(\"normalDataData LengthOfScreenName describe\")\n",
    "print(normalData['LengthOfScreenName'].describe())\n",
    "print(\"\\n\")\n",
    "print(\"normalDataData LengthOfDescriptionInUserProfile describe\")\n",
    "print(normalData['LengthOfDescriptionInUserProfile'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore mergedData dataset seperately\n",
    "print(\"mergedData shape\")\n",
    "print(format(mergedData.shape))\n",
    "print(\"\\n\")\n",
    "print(\"mergedData NumberOfFollowings describe\")\n",
    "print(mergedData['NumberOfFollowings'].describe())\n",
    "print(\"\\n\")\n",
    "print(\"mergedData NumberOfFollowers describe\")\n",
    "print(mergedData['NumberOfFollowers'].describe())\n",
    "print(\"\\n\")\n",
    "print(\"mergedData NumberOfTweets describe\")\n",
    "print(mergedData['NumberOfTweets'].describe())\n",
    "print(\"\\n\")\n",
    "print(\"mergedData LengthOfScreenName describe\")\n",
    "print(mergedData['LengthOfScreenName'].describe())\n",
    "print(\"\\n\")\n",
    "print(\"mergedData LengthOfDescriptionInUserProfile describe\")\n",
    "print(mergedData['LengthOfDescriptionInUserProfile'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Merge together the columns of features for plotting\n",
    "#Let's plot features to determine if anything stands out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove nonnumeric columns\n",
    "mergedData.drop(\"CreatedAt\", axis=1, inplace=True)\n",
    "mergedData.drop(\"CollectedAt\", axis=1, inplace=True)\n",
    "mergedData.drop(\"UserID\", axis=1, inplace=True)\n",
    "mergedData.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#normalize features\n",
    "#mergedDataNorm = mergedData.apply(lambda x: (x-x.min()) / (x.max() - x.min()))\n",
    "#mergedDataNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create feaure columns from our numerical data\n",
    "#numOfFlg = tf.feature_column.numeric_column(\"NumberOfFollowings\")\n",
    "numOfFlr = tf.feature_column.numeric_column(\"NumberOfFollowers\")\n",
    "numOfTw = tf.feature_column.numeric_column(\"NumberOfTweets\")\n",
    "lenScrNm = tf.feature_column.numeric_column(\"LengthOfScreenName\")\n",
    "lenDsPro = tf.feature_column.numeric_column(\"LengthOfDescriptionInUserProfile\")\n",
    "\n",
    "numOfFlr = tf.feature_column.numeric_column('NumberOfFollowings',\n",
    "                    normalizer_fn=lambda x: x - mergedData.describe().mean())\n",
    "feat_columns=[numOfFlr,numOfTw,lenScrNm,lenDsPro,numOfFlr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimator = tf.estimator.LinearClassifer(feature_columns=[numOfFlr,numOfTw,lenScrNm,lenDsPro,numOfFlr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimator.train(input_fn=mergedData,steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdNFlr_mean= badData.mean()\n",
    "print(bdNFlr_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = list(mergedData.columns)\n",
    "\n",
    "feature_names = column_names[:-1]\n",
    "label_name = column_names[-1]\n",
    "\n",
    "print(\"Features: {}\".format(feature_names))\n",
    "print(\"Label: {}\".format(label_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['bot', 'not bot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can try this again with version 2.25 pandas\n",
    "#mergedData.to_csv(\"social_honeypot_icwsm_2011\\trainingData_classified.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size = 32\n",
    "#train_dataset_fp = tf.keras.utils.get_file(fname=\"social_honeypot_icwsm_2011\\mergedData_classified.csv\")\n",
    "#train_dataset = tf.contrib.data.make_csv_dataset(\n",
    "#    mergedData,\n",
    "#    batch_size,\n",
    "#    column_names=column_names,\n",
    "#    label_name=label_name,\n",
    "#    num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(mergedData, test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.2)\n",
    "print(len(train), 'train examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')\n",
    "print(dict(mergedData), 'dict(mergedData)')\n",
    "#print(list(feature_batch.keys()), 'feature_batch.keys()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop('BadUser')\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 # A small batch sized is used for demonstration purposes\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "print(train_ds, \"train_ds\")\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "print(val_ds, \"val_ds\")\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\n",
    "print(test_ds, \"test_ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_batch, label_batch in train_ds.take(1):\n",
    "  print('Every feature:', list(feature_batch.keys()))\n",
    "  print('A batch of NumberOfFollowings:', feature_batch['NumberOfFollowings'])\n",
    "  print('A batch of Bad Users:', label_batch )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility method to create a feature column\n",
    "# and to transform a batch of data\n",
    "def demo(feature_column):\n",
    "  feature_layer = layers.DenseFeatures(feature_column)\n",
    "  print(feature_layer(example_batch).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use this batch to demonstrate several types of feature columns\n",
    "example_batch = next(iter(train_ds))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns.append(feature_column.numeric_column(mergedData.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfFors = feature_column.numeric_column(\"NumberOfFollowers\")\n",
    "demo(numOfFors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  feature_layer,\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'],\n",
    "              run_eagerly=True)\n",
    "\n",
    "model.fit(train_ds,\n",
    "          validation_data=val_ds,\n",
    "          epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print(\"Accuracy\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
